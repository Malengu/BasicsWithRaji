{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "498c8e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U scikit-learn\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "b19e4a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "(\"wine.keys=dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', \"\n",
      " \"'feature_names'])\")\n",
      "================================================================================\n",
      "(\"wine.feature_names=['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', \"\n",
      " \"'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', \"\n",
      " \"'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', \"\n",
      " \"'proline']\")\n",
      "================================================================================\n",
      "('wine.target=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, '\n",
      " '0, 0, 0,\\n'\n",
      " '       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n'\n",
      " '       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\\n'\n",
      " '       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n'\n",
      " '       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n'\n",
      " '       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\\n'\n",
      " '       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\\n'\n",
      " '       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\\n'\n",
      " '       2, 2])')\n",
      "================================================================================\n",
      "wine.target_names=array(['class_0', 'class_1', 'class_2'], dtype='<U7')\n",
      "================================================================================\n",
      "X.shape=(178, 13), y.shape=(178, 1)\n",
      "Xtr.shape=(144, 13), ytr.shape=(144,)\n",
      "Xval.shape=(8, 13), yval.shape=(8,)\n",
      "Xte.shape=(26, 13), yte.shape=(26,)\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# load the wine dataset\n",
    "wine = load_wine()\n",
    "print(\"=\"*80)\n",
    "pprint(f\"wine.keys={wine.keys()}\")\n",
    "print(\"=\"*80)\n",
    "pprint(f\"{wine.feature_names=}\")\n",
    "print(\"=\"*80)\n",
    "pprint(f\"{wine.target=}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{wine.target_names=}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# create a DataFrame from the dataset for easier manipulation\n",
    "wine_df = pd.DataFrame(data=wine.data, columns=wine.feature_names)\n",
    "wine_df['target'] = wine.target\n",
    "\n",
    "# split the dataset into features (X) and target labels (y)\n",
    "X = wine_df.iloc[:, :-1].values # features\n",
    "y = wine_df.iloc[:, -1:].values # target labels\n",
    "\n",
    "print(f\"{X.shape=}, {y.shape=}\")\n",
    "N = X.shape[0]\n",
    "# define the tarin-test split ratio\n",
    "test_size = int((1-0.8089887640449438) * N) # fraction of the test set\n",
    "val_size = test_size // 4 # no validation set for now\n",
    "# shuffle the dataset\n",
    "indices = np.arange(N)\n",
    "np.random.seed(42) # for reproducibility\n",
    "np.random.shuffle(indices)\n",
    "# oops sorry if the indexing is complicated, take a pen and paper and write it down\n",
    "train_indices = indices[test_size:] \n",
    "test_indices = indices[:test_size-val_size]\n",
    "val_indices = indices[test_size-val_size:test_size]\n",
    "Xtr, Xval, Xte = X[train_indices], X[val_indices], X[test_indices]\n",
    "ytr, yval, yte = np.squeeze(y[train_indices], axis=1), np.squeeze(y[val_indices], axis=1), np.squeeze(y[test_indices], axis=1)\n",
    "\n",
    "print(f\"{Xtr.shape=}, {ytr.shape=}\")\n",
    "print(f\"{Xval.shape=}, {yval.shape=}\")\n",
    "print(f\"{Xte.shape=}, {yte.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df6809b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1.shape=(13, 10)\n",
      "b1.shape=(1, 10)\n",
      "W2.shape=(10, 10)\n",
      "b2.shape=(1, 10)\n",
      "W3.shape=(10, 10)\n",
      "b3.shape=(1, 10)\n",
      "W4.shape=(10, 3)\n",
      "total params: 390\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42) # set the random seed for reproducibility\n",
    "\n",
    "# hyperparameters\n",
    "C = 3 # no. of classes\n",
    "Ntr = Xtr.shape[0]\n",
    "feat_in = X.shape[-1] # number of the input features\n",
    "# create simple neural network\n",
    "\n",
    "h1_out = 10 # number of output neurons of the first layer\n",
    "h2_out = 10 # number of output neurons of the second layer\n",
    "h3_out = 10 # number of output neurons of the third layer\n",
    "\n",
    "# layer 1 (hidden layer 1)\n",
    "W1 = np.random.randn(feat_in, h1_out) / math.sqrt(feat_in) # weight for layer 1\n",
    "b1 = np.zeros((1, h1_out))\n",
    "# layer 2 (hidden layer 2)\n",
    "W2 = np.random.randn(h1_out, h2_out) / math.sqrt(h1_out)\n",
    "b2 = np.zeros((1, h2_out))\n",
    "# layer 3 (hidden layer 3)\n",
    "W3 = np.random.randn(h2_out, h3_out) / math.sqrt(h2_out)\n",
    "b3 = np.zeros((1, h3_out))\n",
    "# layer 4 (output layer)\n",
    "W4 = np.random.randn(h3_out, C) / math.sqrt(h3_out)\n",
    "# batch normalization parameters are not used in this simple example\n",
    "gamma1, beta1 = np.ones((1, h1_out)), np.zeros((1, h1_out)) # for layer 1       \n",
    "gamma2, beta2 = np.ones((1, h2_out)), np.zeros((1, h2_out)) # for layer 2\n",
    "gamma3, beta3 = np.ones((1, h3_out)), np.zeros((1, h3_out)) # for layer 3\n",
    "# running buffers for batch normalization (not used in this simple example)\n",
    "running_mean1, running_var1 = np.zeros((1, h1_out)), np.ones((1, h1_out)) # for layer 1\n",
    "running_mean2, running_var2 = np.zeros((1, h2_out)), np.ones((1, h2_out)) # for layer 2\n",
    "running_mean3, running_var3 = np.zeros((1, h3_out)), np.ones((1, h3_out)) # for layer 3\n",
    "# store parameters in a dictionary for easy access\n",
    "params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3': W3, 'b3': b3, 'W4': W4,\n",
    "          'gamma1': gamma1, 'beta1': beta1, 'gamma2': gamma2, 'beta2': beta2, 'gamma3': gamma3, 'beta3': beta3}\n",
    "running_buffers = {'running_mean1': running_mean1, 'running_var1': running_var1,\n",
    "                   'running_mean2': running_mean2, 'running_var2': running_var2,\n",
    "                   'running_mean3': running_mean3, 'running_var3': running_var3}\n",
    "\n",
    "# print the shapes of the parameters\n",
    "print(f\"{W1.shape=}\")\n",
    "print(f\"{b1.shape=}\")\n",
    "print(f\"{W2.shape=}\")\n",
    "print(f\"{b2.shape=}\")\n",
    "print(f\"{W3.shape=}\")\n",
    "print(f\"{b3.shape=}\")\n",
    "print(f\"{W4.shape=}\")\n",
    "\n",
    "# count the total number of parameters\n",
    "print(f\"total params: {sum([np.prod(param.shape) for param in [W1, b1, W2, b2, W3, b3, W4]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9ee1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_normalization(x, params, running_buffers, layer, training=True, momentum=0.1, eps=1e-5):\n",
    "    # swole doge normalization (no libraries!)\n",
    "    B = x.shape[0] # batch size\n",
    "    gamma, beta = params[f'gamma{layer}'], params[f'beta{layer}']\n",
    " \n",
    "    if training:\n",
    "        # check the paper: https://arxiv.org/abs/1502.03167\n",
    "        xmean = (1.0/B) * np.sum(x, axis=0, keepdims=True) # (B, C) -> (1, C) \n",
    "        xvar = (1.0/(B-1)) * np.sum((x - xmean)**2, axis=0, keepdims=True) # (B, C) -> (1, C) note: Bessel's correction (dividing by n-1, not n) https://en.wikipedia.org/wiki/Bessel%27s_correction\n",
    "        # running statistics would be updated here if we were to implement them\n",
    "        running_buffers[f'running_mean{layer}'] = (1.0 - momentum) * running_buffers[f'running_mean{layer}'] + momentum * xmean \n",
    "        running_buffers[f'running_var{layer}'] = (1.0 - momentum) *  running_buffers[f'running_var{layer}'] + momentum * xvar \n",
    "    else:\n",
    "        xmean = running_buffers[f'running_mean{layer}']\n",
    "        xvar = running_buffers[f'running_var{layer}']\n",
    "    # normalize the input    \n",
    "    xhat = (x - xmean) / np.sqrt(xvar + eps) # [(B, C) - (1, C)] / sqrt((1, C) + eps) -> (B, C)\n",
    "    out = gamma * xhat + beta # (B, C) * (1, C) + (1, C) -> (B, C)\n",
    "    cache = (x, xhat, xmean, xvar, gamma, eps)\n",
    "    return out, cache\n",
    "\n",
    "def batch_normalization_backward(dout, layer, cache):\n",
    "    x, xhat, xmean, xvar, gamma, eps = cache  # implement the backward pass for batch normalization\n",
    "    B = x.shape[0]\n",
    "    dgamma = np.sum(dout * xhat, axis=0, keepdims=True) # (B, C) -> (1, C)\n",
    "    dbeta = np.sum(dout, axis=0, keepdims=True) # (B, C) -> (1, C)\n",
    "    dxhat = dout * gamma # (B, C) * (1, C) -> (B, C)\n",
    "    dxvar = np.sum((-1.0 / 2.0) * (x - xmean) * (xvar + eps)**(-3.0/2.0) * dxhat, axis=0, keepdims=True) # (B, C) -> (1, C)\n",
    "    dx = (1.0 / np.sqrt(xvar + eps)) * dxhat # (B, C)\n",
    "    dxmean = np.sum((-1.0 / np.sqrt(xvar + eps)) * dxhat, axis=0, keepdims=True) # (B, C) -> (1, C)\n",
    "    dx += (2.0 / (B-1)) * (x - xmean) * dxvar # (B, C)\n",
    "    dxmean += np.sum((-2.0 / (B-1)) * (x - xmean) * dxvar, axis=0, keepdims=True) # (B, C) -> (1, C) \n",
    "    dx += (1.0 / B) * dxmean # (B, C)\n",
    "    \n",
    "    return dx, {f\"gamma{layer}\": dgamma, f\"beta{layer}\": dbeta}\n",
    "    \n",
    "\n",
    "def forward(X, y, params, reg=0.1):\n",
    "    W1, b1 = params['W1'], params['b1'] # parameters of the first layer\n",
    "    W2, b2 = params['W2'], params['b2'] # parameters of the second layer\n",
    "    W3, b3 = params['W3'], params['b3'] # parameters of the third layer\n",
    "    W4 = params['W4'] # weights of the output layer\n",
    "\n",
    "    cache = {}\n",
    "    # layer 1 forward pass\n",
    "    for i, (wi, bi) in enumerate(zip([W1, W2, W3], [b1, b2, b3])):\n",
    "        hi = np.dot(X, wi) + bi\n",
    "        hi, bn_cache = batch_normalization(hi, params, running_buffers, layer=i+1, training=True, momentum=0.1, eps=1e-5) # batch normalization\n",
    "        ai = np.maximum(hi, 0) # ReLU activation\n",
    "        X = ai # for next layer input\n",
    "        cache[f'h{i+1}'], cache[f'a{i+1}'] = hi, ai\n",
    "        cache = {**cache, f'bn_cache{i+1}': bn_cache}\n",
    "    \n",
    "    logits = np.dot(X, W4) # logits: unnormalized log counts -> (B, h3_out) @ (h3_out, C) -> (B, C)\n",
    "    # softmax implementation with numerical stability (online softmax?)!\n",
    "    shift_logits = logits - np.max(logits, axis=1, keepdims=True) # (B, C)\n",
    "    counts = np.exp(shift_logits)  # (B, C)\n",
    "    norm_factor = np.sum(counts, axis=1, keepdims=True) # (B, C) -> (B, 1)\n",
    "    probs = counts / norm_factor # (B, C)\n",
    "    loss = np.mean(-np.log(probs[range(X.shape[0]),  y])) + 0.5*reg*sum([np.sum(wi**2).item() for wi in [W1, W2, W3, W4]]) # loss with L2 regularization: (B, C) -> c (scalar) \n",
    "    cache = {**cache, **{'logits': logits, 'probs': probs, 'counts': counts, 'norm_factor': norm_factor}} \n",
    "    \n",
    "    return loss, cache\n",
    "\n",
    "def backward(X, y, params, cache, reg):\n",
    "    # initialize the gradients\n",
    "    W2, W3, W4 = params['W2'], params['W3'], params['W4']\n",
    "    h1, a1, h2, a2, h3, a3 = cache['h1'], cache['a1'], cache['h2'], cache['a2'],  cache['h3'], cache['a3']\n",
    "    logits, probs, counts, norm_factor  = cache['logits'], cache['probs'], cache['counts'], cache['norm_factor']\n",
    "    \n",
    "    B = X.shape[0]\n",
    "    layer = 3 # last layer\n",
    "    bn_grads = {}\n",
    "\n",
    "    # softmax backpropagation\n",
    "    dloss = 1.0 # upstream gradient\n",
    "    dW4, dW3, dW2, dW1 = reg*W4, reg*W3, reg*W2, reg*W1 # gradients of weights in L2 regularization weights \n",
    "    dprobs = np.zeros_like(probs) # (B, C)\n",
    "    dprobs[range(B),  y] = dloss * (-1.0/B) * (1.0 / probs[range(B),  y]) # (B,)\n",
    "    dnorm_factor = np.sum(-dprobs * counts * norm_factor**-2, axis=1, keepdims=True)  # (B, 1)\n",
    "    dcounts = dprobs * (norm_factor**-1) # (B, C)\n",
    "    dcounts += 1.0 * dnorm_factor\n",
    "    dshift_logits = dcounts * counts  # (B, C)\n",
    "    dlogits = np.copy(dshift_logits) # (B, C)\n",
    "    # TODO: prove that the line of code below is unnecessary !\n",
    "    dlogits[range(B), np.argmax(logits, axis=1)] += np.sum(-1.0 * dshift_logits, axis=1) # (B, C) -> (B, 1)\n",
    "    # output layer backprogation\n",
    "    dW4 += np.dot(a3.T, dlogits) # (h3_out, B) @ (B, C) -> (h3_out, C)\n",
    "    da3 = np.dot(dlogits, W4.T) # (B, C) @ (C, h3_out) -> (B, h3_out)\n",
    "    # hidden layer 3 backpropagtion \n",
    "    dh3 = da3 * (h3 > 0) # backprop of ReLU -> (B, h3_out)\n",
    "    dbn3, grads = batch_normalization_backward(dh3, layer, cache[f'bn_cache{layer}'])\n",
    "    bn_grads = {**bn_grads, **grads}\n",
    "    layer -= 1\n",
    "    dW3 += np.dot(a2.T, dbn3) # (h2_out, B) @ (B, h3_out) -> (h2_out, h3_out)\n",
    "    da2 = np.dot(dbn3, W3.T) # (B, h3_out) @ (h3_out, h2_out) -> (B, h2_out)\n",
    "    db3 = np.sum(dbn3, axis=0, keepdims=True) # (1, h3_out)\n",
    "    # hidden layer 2 backpropagation\n",
    "    dh2 = da2 * (h2 > 0) # backprop of ReLU -> (B, h2_out)\n",
    "    dbn2, grads = batch_normalization_backward(dh2, layer, cache[f'bn_cache{layer}'])\n",
    "    bn_grads = {**bn_grads, **grads}\n",
    "    layer -= 1\n",
    "    dW2 += np.dot(a1.T, dbn2) # (h1_out, B) @ (B, h2_out) -> (h1_out, h2_out)\n",
    "    da1 = np.dot(dbn2, W2.T) # (B, h2_out) @ (h2_out, h1_out) -> (B, h1_out)\n",
    "    db2 = np.sum(dbn2, axis=0, keepdims=True) # (1, h2_out)\n",
    "    # hidden layer 1 backpropagation\n",
    "    dh1 = da1 * (h1 > 0) # backprop of ReLU -> (B, h1_out)\n",
    "    dbn1, grads = batch_normalization_backward(dh1, layer, cache[f'bn_cache{layer}'])\n",
    "    bn_grads = {**bn_grads, **grads}\n",
    "    dW1 += np.dot(X.T, dbn1) # (feat_in, B) @ (B, h1_out) -> (feat_in, h1_out)\n",
    "    db1 = np.sum(dbn1, axis=0, keepdims=True) # (1, h1_out)\n",
    "    \n",
    "    return {'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2, 'W3': dW3, 'b3': db3, 'W4': dW4, **bn_grads}\n",
    "\n",
    "def numerical_gradient_checking(X, y, params, pname, reg, epsilon=1e-5):\n",
    "    param = params[pname]\n",
    "    grad_approx = np.zeros_like(param)\n",
    "    it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        original_value = param[idx]\n",
    "        \n",
    "        # compute f(x + epsilon)\n",
    "        param[idx] = original_value + epsilon\n",
    "        loss_plus, _ = forward(X, y, params, reg=reg)\n",
    "        \n",
    "        # compute f(x - epsilon)\n",
    "        param[idx] = original_value - epsilon\n",
    "        loss_minus, _ = forward(X, y, params, reg=reg)\n",
    "        \n",
    "        # compute the symmetric derivative: https://en.wikipedia.org/wiki/Symmetric_derivative \n",
    "        grad_approx[idx] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "        \n",
    "        # restore the original value\n",
    "        param[idx] = original_value\n",
    "        it.iternext()\n",
    "    \n",
    "    return grad_approx\n",
    "\n",
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, grad, grad_approx, error=1e-7):\n",
    "    assert np.all(grad - grad_approx < error).item(), f\"({s}) maxdiff: {np.max(np.abs(grad - grad_approx)).item()}\"\n",
    "    assert np.allclose(grad, grad_approx), f\"({s}) maxdiff: {np.max(np.abs(grad - grad_approx)).item()}\"\n",
    "    maxdiff = np.max(np.abs(grad - grad_approx)).item()\n",
    "    print(f'{s:15s} | exact: {str(True):5s} | approximate: {str(True):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "93244e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 00000, loss: 2.55390\n",
      "epoch: 00100, loss: 2.43195\n",
      "epoch: 00200, loss: 2.35206\n",
      "epoch: 00300, loss: 2.21226\n",
      "epoch: 00400, loss: 2.16391\n",
      "epoch: 00500, loss: 2.11757\n",
      "epoch: 00600, loss: 2.03166\n",
      "epoch: 00700, loss: 1.95583\n",
      "epoch: 00800, loss: 1.90066\n",
      "epoch: 00900, loss: 1.84292\n",
      "epoch: 01000, loss: 1.78457\n",
      "epoch: 01100, loss: 1.77150\n",
      "epoch: 01200, loss: 1.78013\n",
      "epoch: 01300, loss: 1.65982\n",
      "epoch: 01400, loss: 1.56614\n",
      "epoch: 01500, loss: 1.57639\n",
      "epoch: 01600, loss: 1.51869\n",
      "epoch: 01700, loss: 1.49146\n",
      "epoch: 01749, loss: 1.51753\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters for training\n",
    "lr = 0.0001\n",
    "reg = 0.1 # L2 regularization strength\n",
    "B = 8\n",
    "num_iter_per_epoch = Ntr // B\n",
    "epochs = 1750 # going through the whole dataset\n",
    "best_val_loss = float('inf')\n",
    "gradient_checking = False\n",
    "lr_decay = False\n",
    "verbose = False\n",
    "# training of simple neural network\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = []\n",
    "    \n",
    "    if lr_decay and (epoch+1) % 100 == 0: lr /= 10.0 # learning rate decay\n",
    "    # shuffle the training set\n",
    "    batch_idxs = np.arange(Ntr)\n",
    "    np.random.shuffle(batch_idxs)\n",
    "    \n",
    "    for it in range(num_iter_per_epoch):\n",
    "        # crux of this whole session!\n",
    "        # index the batch from the pool of shuffled training indices\n",
    "        Xtr_batch = Xtr[batch_idxs[it*B:it*B+B]]\n",
    "        ytr_batch = ytr[batch_idxs[it*B:it*B+B]]\n",
    "\n",
    "        # forward pass\n",
    "        loss, cache = forward(Xtr_batch, ytr_batch, params, reg=reg)\n",
    "        \n",
    "        # backward propagation\n",
    "        gradients = backward(Xtr_batch, ytr_batch, params, cache, reg=reg)\n",
    "        \n",
    "        # TODO: numerical gradient checking\n",
    "        if gradient_checking and epoch % 1000 == 0 and it == 0:\n",
    "            for pname in gradients.keys():\n",
    "                grad_approx = numerical_gradient_checking(Xtr_batch, ytr_batch, params, pname, reg=reg)\n",
    "                cmp(pname, gradients[pname], grad_approx)  \n",
    "    \n",
    "        # update the weights through gradient descent step\n",
    "        for pname in params.keys():\n",
    "            params[pname] -= lr*gradients[pname]\n",
    "        \n",
    "        # validate the model on the validation set\n",
    "        if (epoch == epochs-1 or epoch % 10 == 0) and (it == 0 or it == num_iter_per_epoch - 1):\n",
    "            val_loss, _ = forward(Xval, yval, params, reg=reg)\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                # save the best parameters\n",
    "                best_W1, best_b1 = np.copy(params['W1']), np.copy(params['b1'])\n",
    "                best_W2, best_b2 = np.copy(params['W2']), np.copy(params['b2'])\n",
    "                best_W3, best_b3 = np.copy(params['W3']), np.copy(params['b3'])\n",
    "                best_W4 = np.copy(params['W4'])\n",
    "                # save the best batch normalization parameters\n",
    "                best_gamma1, best_beta1 = np.copy(params['gamma1']), np.copy(params['beta1'])\n",
    "                best_gamma2, best_beta2 = np.copy(params['gamma2']), np.copy(params['beta2'])\n",
    "                best_gamma3, best_beta3 = np.copy(params['gamma3']), np.copy(params['beta3'])\n",
    "        \n",
    "        # print the loss for every iteration\n",
    "        if verbose:\n",
    "            print(f\"epoch: {epoch:02}, iter loss: {loss:.5f}\")\n",
    "        epoch_loss.append(loss)\n",
    "    # log the epoch losses  \n",
    "    if epoch % 100 == 0 or epoch == epochs - 1:\n",
    "        print(f\"epoch: {epoch:05}, loss: {sum(epoch_loss) / num_iter_per_epoch:.5f}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b25ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.15384615384616\n"
     ]
    }
   ],
   "source": [
    "# test / evaluation!\n",
    "def infer():\n",
    "    h1 = np.dot(Xte, best_W1) + best_b1 # (B, C) @ (C, h1_out) -> (B, h1_out) + (h1_out, ) -> (B, h1_out)\n",
    "    norm_params = {'gamma1': best_gamma1, 'beta1': best_beta1}\n",
    "    bn1, _ = batch_normalization(h1, norm_params, running_buffers, layer=1, training=False, momentum=0.1, eps=1e-5) # batch normalization\n",
    "    a1 = np.maximum(bn1, 0) # ReLU activation -> (B, h1_out)\n",
    "    # layer 2 forward pass\n",
    "    h2 = np.dot(a1, best_W2) + best_b2 # (B, h1_out) @ (h1_out, h2_out) + (h2_out, ) -> (B, h2_out)\n",
    "    norm_params = {'gamma2': best_gamma2, 'beta2': best_beta2}\n",
    "    bn2, _ = batch_normalization(h2, norm_params, running_buffers, layer=2, training=False, momentum=0.1, eps=1e-5) # batch normalization\n",
    "    a2 = np.maximum(bn2, 0) # ReLU activation (B, h2_out) -> (B, h2_out)\n",
    "    # layer 3 forward pass \n",
    "    h3 = np.dot(a2, best_W3) + best_b3 # (B, h2_out) @ (h2_out, h3_out) + (h3_out) -> (B, h3_out)\n",
    "    norm_params = {'gamma3': best_gamma3, 'beta3': best_beta3}\n",
    "    bn3, _ = batch_normalization(h3, norm_params, running_buffers, layer=3, training=False, momentum=0.1, eps=1e-5) # batch normalization\n",
    "    a3 = np.maximum(bn3, 0) # ReLU activation (B, h3_out) -> (B, h3_out)\n",
    "    predicts = np.dot(a3, best_W4) #\n",
    "    print(f\"Accuracy: {(sum(np.argmax(predicts, axis=1) == yte) / yte.shape[0]) * 100}\")\n",
    "    \n",
    "infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3e284e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7fbc59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ronald_dit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
